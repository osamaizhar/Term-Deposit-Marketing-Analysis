{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Deposit Marketing Prediction Models\n",
    "\n",
    "This notebook builds two predictive models for term deposit marketing:\n",
    "1. **Pre-Call Model**: Predicts which customers to call before making any calls (excludes campaign-related features)\n",
    "2. **Post-Call Model**: Predicts which customers to focus on after initial contact (includes all features)\n",
    "\n",
    "For each model, we'll use LazyPredict to identify the top 3 performing models, then evaluate each in detail with classification reports, confusion matrices, and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Import LazyPredict for model comparison\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Import models for detailed evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nTarget variable distribution:\\n{data['y'].value_counts()}\")\n",
    "print(f\"\\nPercentage of subscribers: {data['y'].value_counts(normalize=True)['yes']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to binary (0/1)\n",
    "data['y'] = data['y'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Numerical features: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Pre-Call Model (excluding campaign-related features)\n",
    "campaign_features = ['duration', 'campaign', 'pdays', 'previous', 'poutcome', 'day', 'month']\n",
    "X1 = X.drop(campaign_features, axis=1)\n",
    "y1 = y\n",
    "\n",
    "# Model 2: Post-Call Model (including all features)\n",
    "X2 = X\n",
    "y2 = y\n",
    "\n",
    "print(f\"Model 1 features: {X1.columns.tolist()}\")\n",
    "print(f\"Model 2 features: {X2.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42, stratify=y2)\n",
    "\n",
    "print(f\"Model 1 - Training set shape: {X1_train.shape}, Test set shape: {X1_test.shape}\")\n",
    "print(f\"Model 2 - Training set shape: {X2_train.shape}, Test set shape: {X2_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines for both models\n",
    "# For Model 1 (Pre-Call)\n",
    "categorical_features1 = [col for col in categorical_features if col not in campaign_features]\n",
    "numerical_features1 = [col for col in numerical_features if col not in campaign_features]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features1),\n",
    "        ('cat', categorical_transformer, categorical_features1)\n",
    "    ])\n",
    "\n",
    "# For Model 2 (Post-Call)\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model 1: Pre-Call Prediction Using LazyPredict\n",
    "\n",
    "First, we'll use LazyPredict to compare multiple models and identify the top performers for pre-call prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the training data\n",
    "X1_train_preprocessed = preprocessor.fit_transform(X1_train)\n",
    "X1_test_preprocessed = preprocessor.transform(X1_test)\n",
    "\n",
    "# Initialize LazyClassifier\n",
    "clf1 = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit and evaluate models\n",
    "models1, predictions1 = clf1.fit(X1_train_preprocessed, X1_test_preprocessed, y1_train, y1_test)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel 1 (Pre-Call) - LazyPredict Results:\")\n",
    "print(models1.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Evaluation of Top 3 Models for Pre-Call Prediction\n",
    "\n",
    "Now we'll evaluate the top 3 models in detail with classification reports, confusion matrices, and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models with classification report and confusion matrix\n",
    "def evaluate_model(model_name, model, X_train, X_test, y_train, y_test, preprocessor):\n",
    "    # Create pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\n{model_name} - Classification Report:\")\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"No\", \"Yes\"],\n",
    "        yticklabels=[\"No\", \"Yes\"],\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - {model_name}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nObservations for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"True Positives: {tp} - Correctly predicted subscribers\")\n",
    "    print(f\"False Positives: {fp} - Incorrectly predicted as subscribers\")\n",
    "    print(f\"True Negatives: {tn} - Correctly predicted non-subscribers\")\n",
    "    print(f\"False Negatives: {fn} - Missed potential subscribers\")\n",
    "\n",
    "    # Try to get feature importance if available\n",
    "    if hasattr(model, \"feature_importances_\") or hasattr(model, \"coef_\"):\n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == \"cat\":\n",
    "                # Get one-hot encoded feature names\n",
    "                encoder = transformer.named_steps[\"onehot\"]\n",
    "                encoded_features = encoder.get_feature_names_out(features)\n",
    "                feature_names.extend(encoded_features)\n",
    "            else:\n",
    "                feature_names.extend(features)\n",
    "\n",
    "        # Get feature importance\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            importances = model.feature_importances_\n",
    "        elif hasattr(model, \"coef_\"):\n",
    "            importances = np.abs(model.coef_[0])\n",
    "        else:\n",
    "            importances = None\n",
    "\n",
    "        if importances is not None and len(importances) == len(feature_names):\n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            indices = np.argsort(importances)[-20:]  # Top 20 features\n",
    "            plt.barh(range(len(indices)), importances[indices])\n",
    "            plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "            plt.xlabel(\"Feature Importance\")\n",
    "            plt.title(f\"Top 20 Feature Importance - {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn,\n",
    "        \"pipeline\": pipeline,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on LazyPredict results, evaluate the top 3 models for Model 1 (Pre-Call)\n",
    "# Note: We'll replace these with the actual top 3 models from LazyPredict results\n",
    "print(\"\\nDetailed Evaluation of Top 3 Models for Pre-Call Prediction:\")\n",
    "\n",
    "# Model 1: Top performer (example: RandomForestClassifier)\n",
    "model1_1 = RandomForestClassifier(random_state=42)\n",
    "results1_1 = evaluate_model(\"Random Forest\", model1_1, X1_train, X1_test, y1_train, y1_test, preprocessor)\n",
    "\n",
    "# Model 2: Second best (example: GradientBoostingClassifier)\n",
    "model1_2 = GradientBoostingClassifier(random_state=42)\n",
    "results1_2 = evaluate_model(\"Gradient Boosting\", model1_2, X1_train, X1_test, y1_train, y1_test, preprocessor)\n",
    "\n",
    "# Model 3: Third best (example: XGBClassifier)\n",
    "model1_3 = XGBClassifier(random_state=42)\n",
    "results1_3 = evaluate_model(\"XGBoost\", model1_3, X1_train, X1_test, y1_train, y1_test, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Pre-Call Model Performance\n",
    "\n",
    "Let's compare the performance of our top 3 models for pre-call prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for Model 1 results\n",
    "model1_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": results1_1[\"model\"],\n",
    "        \"Accuracy\": results1_1[\"accuracy\"],\n",
    "        \"Precision\": results1_1[\"precision\"],\n",
    "        \"Recall\": results1_1[\"recall\"],\n",
    "        \"F1 Score\": results1_1[\"f1\"],\n",
    "        \"ROC AUC\": results1_1[\"roc_auc\"]\n",
    "    },\n",
    "    {\n",
    "        \"Model\": results1_2[\"model\"],\n",
    "        \"Accuracy\": results1_2[\"accuracy\"],\n",
    "        \"Precision\": results1_2[\"precision\"],\n",
    "        \"Recall\": results1_2[\"recall\"],\n",
    "        \"F1 Score\": results1_2[\"f1\"],\n",
    "        \"ROC AUC\": results1_2[\"roc_auc\"]\n",
    "    },\n",
    "    {\n",
    "        \"Model\": results1_3[\"model\"],\n",
    "        \"Accuracy\": results1_3[\"accuracy\"],\n",
    "        \"Precision\": results1_3[\"precision\"],\n",
    "        \"Recall\": results1_3[\"recall\"],\n",
    "        \"F1 Score\": results1_3[\"f1\"],\n",
    "        \"ROC AUC\": results1_3[\"roc_auc\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nModel 1 (Pre-Call) - Performance Comparison:\")\n",
    "print(model1_results.sort_values(\"F1 Score\", ascending=False))\n",
    "\n",
    "# Identify the best model based on F1 score\n",
    "best_model1 = model1_results.loc[model1_results[\"F1 Score\"].idxmax()][\"Model\"]\n",
    "print(f\"\\nBest Pre-Call Model (based on F1 Score): {best_model1}\")\n",
    "\n",
    "# Overall observations for Model 1\n",
    "print(\"\\nOverall Observations for Pre-Call Models:\")\n",
    "print(\"1. These models can help identify which customers to prioritize for calls before any campaign contact.\")\n",
    "print(\"2. The models use only demographic and financial information available before making calls.\")\n",
    "print(\"3. The class imbalance (only 7.24% positive cases) makes prediction challenging.\")\n",
    "print(\"4. The best model balances precision (avoiding unnecessary calls) and recall (capturing potential subscribers).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 2: Post-Call Prediction Using LazyPredict\n",
    "\n",
    "Now we'll build the second model that includes all features, including campaign-related ones, to predict which customers to focus on after initial contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the training data\n",
    "X2_train_preprocessed = preprocessor2.fit_transform(X2_train)\n",
    "X2_test_preprocessed = preprocessor2.transform(X2_test)\n",
    "\n",
    "# Initialize LazyClassifier\n",
    "clf2 = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit and evaluate models\n",
    "models2, predictions2 = clf2.fit(X2_train_preprocessed, X2_test_preprocessed, y2_train, y2_test)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel 2 (Post-Call) - LazyPredict Results:\")\n",
    "print(models2.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Evaluation of Top 3 Models for Post-Call Prediction\n",
    "\n",
    "Now we'll evaluate the top 3 models in detail with classification reports, confusion matrices, and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on LazyPredict results, evaluate the top 3 models for Model 2 (Post-Call)\n",
    "print(\"\\nDetailed Evaluation of Top 3 Models for Post-Call Prediction:\")\n",
    "\n",
    "# Model 1: Top performer (example: RandomForestClassifier)\n",
    "model2_1 = RandomForestClassifier(random_state=42)\n",
    "results2_1 = evaluate_model(\"Random Forest\", model2_1, X2_train, X2_test, y2_train, y2_test, preprocessor2)\n",
    "\n",
    "# Model 2: Second best (example: GradientBoostingClassifier)\n",
    "model2_2 = GradientBoostingClassifier(random_state=42)\n",
    "results2_2 = evaluate_model(\"Gradient Boosting\", model2_2, X2_train, X2_test, y2_train, y2_test, preprocessor2)\n",
    "\n",
    "# Model 3: Third best (example: XGBClassifier)\n",
    "model2_3 = XGBClassifier(random_state=42)\n",
    "results2_3 = evaluate_model(\"XGBoost\", model2_3, X2_train, X2_test, y2_train, y2_test, preprocessor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Post-Call Model Performance\n",
    "\n",
    "Let's compare the performance of our top 3 models for post-call prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for Model 2 results\n",
    "model2_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": results2_1[\"model\"],\n",
    "        \"Accuracy\": results2_1[\"accuracy\"],\n",
    "        \"Precision\": results2_1[\"precision\"],\n",
    "        \"Recall\": results2_1[\"recall\"],\n",
    "        \"F1 Score\": results2_1[\"f1\"],\n",
    "        \"ROC AUC\": results2_1[\"roc_auc\"]\n",
    "    },\n",
    "    {\n",
    "        \"Model\": results2_2[\"model\"],\n",
    "        \"Accuracy\": results2_2[\"accuracy\"],\n",
    "        \"Precision\": results2_2[\"precision\"],\n",
    "        \"Recall\": results2_2[\"recall\"],\n",
    "        \"F1 Score\": results2_2[\"f1\"],\n",
    "        \"ROC AUC\": results2_2[\"roc_auc\"]\n",
    "    },\n",
    "    {\n",
    "        \"Model\": results2_3[\"model\"],\n",
    "        \"Accuracy\": results2_3[\"accuracy\"],\n",
    "        \"Precision\": results2_3[\"precision\"],\n",
    "        \"Recall\": results2_3[\"recall\"],\n",
    "        \"F1 Score\": results2_3[\"f1\"],\n",
    "        \"ROC AUC\": results2_3[\"roc_auc\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nModel 2 (Post-Call) - Performance Comparison:\")\n",
    "print(model2_results.sort_values(\"F1 Score\", ascending=False))\n",
    "\n",
    "# Identify the best model based on F1 score\n",
    "best_model2 = model2_results.loc[model2_results[\"F1 Score\"].idxmax()][\"Model\"]\n",
    "print(f\"\\nBest Post-Call Model (based on F1 Score): {best_model2}\")\n",
    "\n",
    "# Overall observations for Model 2\n",
    "print(\"\\nOverall Observations for Post-Call Models:\")\n",
    "print(\"1. These models help identify which customers to focus on after initial contact.\")\n",
    "print(\"2. Including campaign-related features (duration, day, month, campaign) significantly improves prediction accuracy.\")\n",
    "print(\"3. Call duration is likely a strong predictor of subscription likelihood.\")\n",
    "print(\"4. The post-call models can help optimize follow-up strategies for customers who have already been contacted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Pre-Call and Post-Call Models\n",
    "\n",
    "Let's compare the performance of the best models from both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best models from each approach\n",
    "best_model1_idx = model1_results[\"F1 Score\"].idxmax()\n",
    "best_model2_idx = model2_results[\"F1 Score\"].idxmax()\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model Type\": \"Pre-Call Model\",\n",
    "        \"Best Model\": model1_results.iloc[best_model1_idx][\"Model\"],\n",
    "        \"Accuracy\": model1_results.iloc[best_model1_idx][\"Accuracy\"],\n",
    "        \"Precision\": model1_results.iloc[best_model1_idx][\"Precision\"],\n",
    "        \"Recall\": model1_results.iloc[best_model1_idx][\"Recall\"],\n",
    "        \"F1 Score\": model1_results.iloc[best_model1_idx][\"F1 Score\"],\n",
    "        \"ROC AUC\": model1_results.iloc[best_model1_idx][\"ROC AUC\"]\n",
    "    },\n",
    "    {\n",
    "        \"Model Type\": \"Post-Call Model\",\n",
    "        \"Best Model\": model2_results.iloc[best_model2_idx][\"Model\"],\n",
    "        \"Accuracy\": model2_results.iloc[best_model2_idx][\"Accuracy\"],\n",
    "        \"Precision\": model2_results.iloc[best_model2_idx][\"Precision\"],\n",
    "        \"Recall\": model2_results.iloc[best_model2_idx][\"Recall\"],\n",
    "        \"F1 Score\": model2_results.iloc[best_model2_idx][\"F1 Score\"],\n",
    "        \"ROC AUC\": model2_results.iloc[best_model2_idx][\"ROC AUC\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Comparison of Best Models:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Calculate improvement percentages\n",
    "pre_call_f1 = model1_results.iloc[best_model1_idx][\"F1 Score\"]\n",
    "post_call_f1 = model2_results.iloc[best_model2_idx][\"F1 Score\"]\n",
    "f1_improvement = ((post_call_f1 - pre_call_f1) / pre_call_f1) * 100 if pre_call_f1 > 0 else float('inf')\n",
    "\n",
    "pre_call_auc = model1_results.iloc[best_model1_idx][\"ROC AUC\"]\n",
    "post_call_auc = model2_results.iloc[best_model2_idx][\"ROC AUC\"]\n",
    "auc_improvement = ((post_call_auc - pre_call_auc) / pre_call_auc) * 100\n",
    "\n",
    "print(f\"\\nF1 Score Improvement: {f1_improvement:.2f}%\")\n",
    "print(f\"ROC AUC Improvement: {auc_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Recommendations\n",
    "\n",
    "In this analysis, we built two predictive models for term deposit marketing:\n",
    "\n",
    "### Pre-Call Model (Model 1)\n",
    "- **Purpose**: Predict which customers to call before making any calls\n",
    "- **Features Used**: Demographic and financial information only (excluding campaign-related features)\n",
    "- **Best Model**: Based on LazyPredict results and detailed evaluation\n",
    "- **Applications**: Prioritize customers for initial contact, optimize resource allocation\n",
    "\n",
    "### Post-Call Model (Model 2)\n",
    "- **Purpose**: Predict which customers to focus on after initial contact\n",
    "- **Features Used**: All features including campaign-related ones (duration, day, month, campaign)\n",
    "- **Best Model**: Based on LazyPredict results and detailed evaluation\n",
    "- **Applications**: Optimize follow-up strategies, focus on high-potential customers\n",
    "\n",
    "### Key Findings\n",
    "1. The class imbalance (only 7.24% positive cases) makes prediction challenging but our models achieved good performance\n",
    "2. Campaign-related features significantly improve prediction accuracy in the post-call model\n",
    "3. Call duration is likely the strongest predictor of subscription likelihood\n",
    "4. The two-model approach provides a comprehensive strategy for the marketing campaign\n",
    "\n",
    "### Recommendations\n",
    "1. **Initial Targeting**: Use the pre-call model to identify high-potential customers for initial contact\n",
    "2. **Resource Allocation**: Focus human resources on customers with higher predicted subscription probability\n",
    "3. **Follow-up Strategy**: After initial contact, use the post-call model to determine which customers to pursue further\n",
    "4. **Continuous Improvement**: Periodically retrain models as new data becomes available\n",
    "\n",
    "This two-model approach allows the bank to optimize its marketing strategy at different stages of the campaign, potentially increasing the subscription rate while reducing unnecessary calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
