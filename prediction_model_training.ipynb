{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Deposit Marketing Prediction Models\n",
    "\n",
    "This notebook builds two predictive models for term deposit marketing:\n",
    "1. **Pre-Call Model**: Predicts which customers to call before making any calls (excludes campaign-related features)\n",
    "2. **Post-Call Model**: Predicts which customers to focus on after initial contact (includes all features)\n",
    "\n",
    "For each model, we'll use Pycaret to identify the top 3 performing models, then evaluate each in detail with classification reports, confusion matrices, and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Import PyCaret for model comparison (replacing LazyPredict)\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "\n",
    "# Import models for detailed evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Import sampling techniques for class imbalance\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('term-deposit-marketing-2020.csv')\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {data.columns.tolist()}\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nTarget variable distribution:\\n{data['y'].value_counts()}\")\n",
    "print(f\"\\nPercentage of subscribers: {data['y'].value_counts(normalize=True)['yes']*100:.2f}%\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\\n{data.isnull().sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDataset info:\")\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to binary (0/1)\n",
    "data['y'] = data['y'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "\n",
    "# Analyze class imbalance\n",
    "class_distribution = y.value_counts(normalize=True)\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"No subscription (0): {class_distribution[0]*100:.2f}%\")\n",
    "print(f\"Subscription (1): {class_distribution[1]*100:.2f}%\")\n",
    "print(f\"\\nClass imbalance ratio: {class_distribution[0]/class_distribution[1]:.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define campaign-related features to exclude from Model 1\n",
    "# These features are only available AFTER making calls\n",
    "campaign_features = ['duration', 'campaign', 'day', 'month']\n",
    "\n",
    "# Check which campaign features actually exist in our dataset\n",
    "available_campaign_features = [f for f in campaign_features if f in X.columns]\n",
    "print(f\"Available campaign features to exclude: {available_campaign_features}\")\n",
    "\n",
    "# Model 1: Pre-Call Model (excluding campaign-related features)\n",
    "X1 = X.drop(available_campaign_features, axis=1, errors='ignore')\n",
    "y1 = y\n",
    "\n",
    "# Model 2: Post-Call Model (including all features)\n",
    "X2 = X\n",
    "y2 = y\n",
    "\n",
    "print(f\"\\nModel 1 (Pre-Call) features ({len(X1.columns)}): {X1.columns.tolist()}\")\n",
    "print(f\"\\nModel 2 (Post-Call) features ({len(X2.columns)}): {X2.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nFeatures excluded from Model 1: {available_campaign_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for both models\n",
    "# Model 1: Pre-Call Dataset\n",
    "data1 = X1.copy()\n",
    "data1['y'] = y1\n",
    "\n",
    "# Model 2: Post-Call Dataset  \n",
    "data2 = X2.copy()\n",
    "data2['y'] = y2\n",
    "\n",
    "print(f\"Model 1 (Pre-Call) dataset shape: {data1.shape}\")\n",
    "print(f\"Model 2 (Post-Call) dataset shape: {data2.shape}\")\n",
    "\n",
    "# Split data for traditional sklearn approach (backup)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42, stratify=y2)\n",
    "\n",
    "print(f\"\\nTraditional split:\")\n",
    "print(f\"Model 1 - Training: {X1_train.shape}, Test: {X1_test.shape}\")\n",
    "print(f\"Model 2 - Training: {X2_train.shape}, Test: {X2_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing function for sklearn models\n",
    "def preprocess_data(X_train, X_test, categorical_cols, numerical_cols):\n",
    "    \"\"\"\n",
    "    Simple preprocessing for sklearn models\n",
    "    \"\"\"\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # Handle categorical variables with label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_processed.columns:\n",
    "            le = LabelEncoder()\n",
    "            X_train_processed[col] = le.fit_transform(X_train_processed[col].astype(str))\n",
    "            X_test_processed[col] = le.transform(X_test_processed[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Handle numerical variables\n",
    "    scaler = StandardScaler()\n",
    "    if numerical_cols:\n",
    "        num_cols_present = [col for col in numerical_cols if col in X_train_processed.columns]\n",
    "        if num_cols_present:\n",
    "            X_train_processed[num_cols_present] = scaler.fit_transform(X_train_processed[num_cols_present])\n",
    "            X_test_processed[num_cols_present] = scaler.transform(X_test_processed[num_cols_present])\n",
    "    \n",
    "    return X_train_processed, X_test_processed, label_encoders, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model 1: Pre-Call Prediction Using PyCaret\n",
    "\n",
    "First, we'll use PyCaret to compare multiple models and identify the top performers for pre-call prediction. This model will help identify which customers to call BEFORE making any calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyCaret environment for Model 1 (Pre-Call)\n",
    "print(\"Setting up PyCaret for Model 1 (Pre-Call Prediction)...\")\n",
    "print(f\"Dataset shape: {data1.shape}\")\n",
    "print(f\"Target distribution:\\n{data1['y'].value_counts()}\")\n",
    "\n",
    "# Setup PyCaret classification environment\n",
    "clf1 = setup(\n",
    "    data=data1,\n",
    "    target='y',\n",
    "    session_id=123,\n",
    "    train_size=0.8,\n",
    "    silent=True,\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "print(\"\\nPyCaret setup completed for Model 1!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models using PyCaret\n",
    "print(\"Comparing multiple models for Pre-Call Prediction...\")\n",
    "models1_comparison = compare_models(\n",
    "    include=['lr', 'rf', 'et', 'gbc', 'xgboost', 'lightgbm', 'ada', 'dt', 'nb'],\n",
    "    sort='F1',\n",
    "    n_select=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 models for Pre-Call Prediction (sorted by F1 Score):\")\n",
    "print(models1_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Evaluation of Top 3 Models for Pre-Call Prediction\n",
    "\n",
    "Now we'll evaluate the top 3 models in detail with classification reports, confusion matrices, and ROC curves. We'll focus on business metrics: retaining subscribers while avoiding unnecessary calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate PyCaret models with detailed metrics and confusion matrix\n",
    "def evaluate_pycaret_model(model_name, model, test_data=None):\n",
    "    \"\"\"\n",
    "    Evaluate a PyCaret model with detailed business-focused metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    if test_data is not None:\n",
    "        predictions = predict_model(model, data=test_data)\n",
    "        y_true = test_data['y']\n",
    "        y_pred = predictions['prediction_label']\n",
    "        y_pred_proba = predictions['prediction_score']\n",
    "    else:\n",
    "        # Use holdout set\n",
    "        predictions = predict_model(model)\n",
    "        y_true = predictions['y']\n",
    "        y_pred = predictions['prediction_label']\n",
    "        y_pred_proba = predictions['prediction_score']\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Business metrics\n",
    "    total_customers = len(y_true)\n",
    "    actual_subscribers = sum(y_true)\n",
    "    predicted_to_call = sum(y_pred)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['No Subscription', 'Subscription']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Subscription', 'Subscription'],\n",
    "                yticklabels=['No Subscription', 'Subscription'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Business interpretation\n",
    "    print(f\"\\nBUSINESS METRICS:\")\n",
    "    print(f\"Total Customers: {total_customers:,}\")\n",
    "    print(f\"Actual Subscribers: {actual_subscribers:,} ({actual_subscribers/total_customers*100:.2f}%)\")\n",
    "    print(f\"Predicted to Call: {predicted_to_call:,} ({predicted_to_call/total_customers*100:.2f}%)\")\n",
    "    print(f\"\\nCONFUSION MATRIX BREAKDOWN:\")\n",
    "    print(f\"True Positives (TP): {tp:,} - Correctly identified subscribers\")\n",
    "    print(f\"False Positives (FP): {fp:,} - Unnecessary calls (cost to company)\")\n",
    "    print(f\"True Negatives (TN): {tn:,} - Correctly avoided non-subscribers\")\n",
    "    print(f\"False Negatives (FN): {fn:,} - Missed potential subscribers (lost revenue)\")\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE METRICS:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%) - Of predicted subscribers, how many actually subscribed\")\n",
    "    print(f\"Recall: {recall:.4f} ({recall*100:.2f}%) - Of actual subscribers, how many we identified\")\n",
    "    print(f\"F1 Score: {f1:.4f} - Balance between precision and recall\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f} - Overall model performance\")\n",
    "    \n",
    "    # Business impact\n",
    "    if predicted_to_call > 0:\n",
    "        call_efficiency = tp / predicted_to_call\n",
    "        print(f\"\\nBUSINESS IMPACT:\")\n",
    "        print(f\"Call Efficiency: {call_efficiency:.4f} ({call_efficiency*100:.2f}%) - Success rate of calls\")\n",
    "        print(f\"Subscriber Capture Rate: {recall:.4f} ({recall*100:.2f}%) - % of subscribers we'll reach\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    print(f\"\\nPERFORMANCE ASSESSMENT:\")\n",
    "    if accuracy >= 0.80:\n",
    "        print(f\"‚úÖ EXCELLENT: Accuracy {accuracy*100:.1f}% meets high performance target (‚â•80%)\")\n",
    "    elif accuracy >= 0.75:\n",
    "        print(f\"‚úÖ GOOD: Accuracy {accuracy*100:.1f}% meets target range (75-80%)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  NEEDS IMPROVEMENT: Accuracy {accuracy*100:.1f}% below target (<75%)\")\n",
    "        print(f\"   Consider addressing class imbalance with sampling techniques\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,\n",
    "        'total_customers': total_customers,\n",
    "        'actual_subscribers': actual_subscribers,\n",
    "        'predicted_to_call': predicted_to_call\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate top 3 models for Pre-Call Prediction\n",
    "print(\"\\nCreating and evaluating top 3 models for Pre-Call Prediction...\")\n",
    "\n",
    "# Create top 3 models based on comparison results\n",
    "# You can modify these based on the actual comparison results\n",
    "top_models_1 = ['rf', 'gbc', 'xgboost']  # Modify based on comparison results\n",
    "model1_results = []\n",
    "\n",
    "for i, model_id in enumerate(top_models_1, 1):\n",
    "    print(f\"\\nCreating Model {i}: {model_id}\")\n",
    "    model = create_model(model_id, verbose=False)\n",
    "    \n",
    "    # Finalize the model (trains on full dataset)\n",
    "    final_model = finalize_model(model)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model_name = f\"Model {i} ({model_id.upper()})\"\n",
    "    results = evaluate_pycaret_model(model_name, final_model)\n",
    "    model1_results.append(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Pre-Call Model Performance\n",
    "\n",
    "Let's compare the performance of our top 3 models for pre-call prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame for Model 1 results\n",
    "model1_summary = pd.DataFrame(model1_results)\n",
    "model1_summary = model1_summary.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1 (PRE-CALL) - PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(model1_summary[['model_name', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']].round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model1 = model1_summary.iloc[0]\n",
    "print(f\"\\nüèÜ BEST PRE-CALL MODEL: {best_model1['model_name']}\")\n",
    "print(f\"   F1 Score: {best_model1['f1_score']:.4f}\")\n",
    "print(f\"   Accuracy: {best_model1['accuracy']:.4f} ({best_model1['accuracy']*100:.1f}%)\")\n",
    "\n",
    "# Business insights\n",
    "print(f\"\\nüìä BUSINESS INSIGHTS FOR PRE-CALL MODELS:\")\n",
    "print(f\"1. üéØ PURPOSE: Identify which customers to call BEFORE making any campaign contact\")\n",
    "print(f\"2. üìã FEATURES: Uses only demographic and financial data (no campaign features)\")\n",
    "print(f\"3. ‚öñÔ∏è  CLASS IMBALANCE: Only ~{best_model1['actual_subscribers']/best_model1['total_customers']*100:.1f}% of customers subscribe\")\n",
    "print(f\"4. üí∞ BUSINESS VALUE: Reduces unnecessary calls while capturing potential subscribers\")\n",
    "print(f\"5. üéØ CALL EFFICIENCY: {best_model1['tp']/(best_model1['tp']+best_model1['fp'])*100:.1f}% of predicted calls will be successful\")\n",
    "\n",
    "# Performance assessment\n",
    "avg_accuracy = model1_summary['accuracy'].mean()\n",
    "if avg_accuracy >= 0.75:\n",
    "    print(f\"\\n‚úÖ PERFORMANCE STATUS: Models meet target performance (avg accuracy: {avg_accuracy*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  PERFORMANCE STATUS: Consider class imbalance techniques (avg accuracy: {avg_accuracy*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Analysis and Handling\n",
    "\n",
    "If the models don't achieve 75-80% accuracy, we'll address the class imbalance using sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we need to address class imbalance\n",
    "need_balancing = avg_accuracy < 0.75\n",
    "\n",
    "if need_balancing:\n",
    "    print(\"‚ö†Ô∏è  ADDRESSING CLASS IMBALANCE\")\n",
    "    print(\"Current performance is below 75% target. Applying sampling techniques...\")\n",
    "    \n",
    "    # Setup PyCaret with class balancing\n",
    "    clf1_balanced = setup(\n",
    "        data=data1,\n",
    "        target='y',\n",
    "        session_id=124,\n",
    "        train_size=0.8,\n",
    "        silent=True,\n",
    "        use_gpu=False,\n",
    "        fix_imbalance=True,  # This applies SMOTE\n",
    "        fix_imbalance_method='smote'\n",
    "    )\n",
    "    \n",
    "    # Compare models with balanced data\n",
    "    print(\"\\nComparing models with balanced dataset...\")\n",
    "    models1_balanced = compare_models(\n",
    "        include=['rf', 'gbc', 'xgboost', 'lr', 'et'],\n",
    "        sort='F1',\n",
    "        n_select=3,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate best balanced model\n",
    "    best_balanced_model = create_model('rf', verbose=False)  # or top from comparison\n",
    "    final_balanced_model = finalize_model(best_balanced_model)\n",
    "    balanced_results = evaluate_pycaret_model(\"Balanced Random Forest\", final_balanced_model)\n",
    "    \n",
    "    print(f\"\\nüìà IMPROVEMENT WITH BALANCING:\")\n",
    "    print(f\"Original Accuracy: {avg_accuracy*100:.1f}%\")\n",
    "    print(f\"Balanced Accuracy: {balanced_results['accuracy']*100:.1f}%\")\n",
    "    print(f\"Improvement: {(balanced_results['accuracy'] - avg_accuracy)*100:.1f} percentage points\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ CLASS BALANCE: Current performance meets targets. No balancing needed.\")\n",
    "    print(f\"Average accuracy: {avg_accuracy*100:.1f}% (Target: ‚â•75%)\")\n",
    "\n",
    "print(f\"\\nüí° WHY ONLY {data1['y'].mean()*100:.1f}% SUBSCRIBE?\")\n",
    "print(\"Possible reasons for low subscription rate:\")\n",
    "print(\"1. üìû Cold calling - customers not expecting calls\")\n",
    "print(\"2. üí∞ Economic factors - customers may not have disposable income\")\n",
    "print(\"3. üéØ Targeting - may not be reaching the right customer segments\")\n",
    "print(\"4. üìã Product fit - term deposits may not meet customer needs\")\n",
    "print(\"5. ‚è∞ Timing - calls may be at inconvenient times\")\n",
    "print(\"6. üè¶ Trust - customers may be hesitant about financial products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 2: Post-Call Prediction Using PyCaret\n",
    "\n",
    "Now we'll build the second model that includes all features, including campaign-related ones, to predict which customers to focus on after initial contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyCaret environment for Model 2 (Post-Call)\n",
    "print(\"Setting up PyCaret for Model 2 (Post-Call Prediction)...\")\n",
    "print(f\"Dataset shape: {data2.shape}\")\n",
    "print(f\"Target distribution:\\n{data2['y'].value_counts()}\")\n",
    "\n",
    "# Setup PyCaret classification environment for Model 2\n",
    "clf2 = setup(\n",
    "    data=data2,\n",
    "    target='y',\n",
    "    session_id=125,\n",
    "    train_size=0.8,\n",
    "    silent=True,\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "print(\"\\nPyCaret setup completed for Model 2!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models using PyCaret for Model 2\n",
    "print(\"Comparing multiple models for Post-Call Prediction...\")\n",
    "models2_comparison = compare_models(\n",
    "    include=['lr', 'rf', 'et', 'gbc', 'xgboost', 'lightgbm', 'ada', 'dt', 'nb'],\n",
    "    sort='F1',\n",
    "    n_select=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 models for Post-Call Prediction (sorted by F1 Score):\")\n",
    "print(models2_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Evaluation of Top 3 Models for Post-Call Prediction\n",
    "\n",
    "Now we'll evaluate the top 3 models in detail with classification reports, confusion matrices, and business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate top 3 models for Post-Call Prediction\n",
    "print(\"\\nCreating and evaluating top 3 models for Post-Call Prediction...\")\n",
    "\n",
    "# Create top 3 models based on comparison results\n",
    "top_models_2 = ['rf', 'gbc', 'xgboost']  # Modify based on comparison results\n",
    "model2_results = []\n",
    "\n",
    "for i, model_id in enumerate(top_models_2, 1):\n",
    "    print(f\"\\nCreating Model {i}: {model_id}\")\n",
    "    model = create_model(model_id, verbose=False)\n",
    "    \n",
    "    # Finalize the model (trains on full dataset)\n",
    "    final_model = finalize_model(model)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model_name = f\"Model {i} ({model_id.upper()})\"\n",
    "    results = evaluate_pycaret_model(model_name, final_model)\n",
    "    model2_results.append(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Post-Call Model Performance\n",
    "\n",
    "Let's compare the performance of our top 3 models for post-call prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame for Model 2 results\n",
    "model2_summary = pd.DataFrame(model2_results)\n",
    "model2_summary = model2_summary.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2 (POST-CALL) - PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(model2_summary[['model_name', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']].round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model2 = model2_summary.iloc[0]\n",
    "print(f\"\\nüèÜ BEST POST-CALL MODEL: {best_model2['model_name']}\")\n",
    "print(f\"   F1 Score: {best_model2['f1_score']:.4f}\")\n",
    "print(f\"   Accuracy: {best_model2['accuracy']:.4f} ({best_model2['accuracy']*100:.1f}%)\")\n",
    "\n",
    "# Business insights\n",
    "print(f\"\\nüìä BUSINESS INSIGHTS FOR POST-CALL MODELS:\")\n",
    "print(f\"1. üéØ PURPOSE: Optimize follow-up after initial customer contact\")\n",
    "print(f\"2. üìã FEATURES: Includes ALL features including campaign data (duration, timing)\")\n",
    "print(f\"3. üìà PERFORMANCE: Should outperform pre-call models due to additional features\")\n",
    "print(f\"4. üí∞ BUSINESS VALUE: Focus resources on customers most likely to convert\")\n",
    "print(f\"5. üéØ CALL EFFICIENCY: {best_model2['tp']/(best_model2['tp']+best_model2['fp'])*100:.1f}% of predicted calls will be successful\")\n",
    "\n",
    "# Feature impact analysis\n",
    "print(f\"\\nüîç CAMPAIGN FEATURE IMPACT:\")\n",
    "print(f\"‚Ä¢ Call Duration: Likely strongest predictor of subscription\")\n",
    "print(f\"‚Ä¢ Call Timing: Day/month may affect customer receptiveness\")\n",
    "print(f\"‚Ä¢ Campaign Count: Number of contacts may indicate interest level\")\n",
    "print(f\"‚Ä¢ Previous Outcome: Historical response patterns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Pre-Call and Post-Call Models\n",
    "\n",
    "Let's compare the performance of the best models from both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison between best models from both approaches\n",
    "model1_summary_df = pd.DataFrame(model1_results)\n",
    "model2_summary_df = pd.DataFrame(model2_results)\n",
    "\n",
    "# Get best models (highest F1 score)\n",
    "best_model1 = model1_summary_df.loc[model1_summary_df['f1_score'].idxmax()]\n",
    "best_model2 = model2_summary_df.loc[model2_summary_df['f1_score'].idxmax()]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model Type\": \"Pre-Call Model\",\n",
    "        \"Best Model\": best_model1['model_name'],\n",
    "        \"Accuracy\": best_model1['accuracy'],\n",
    "        \"Precision\": best_model1['precision'],\n",
    "        \"Recall\": best_model1['recall'],\n",
    "        \"F1 Score\": best_model1['f1_score'],\n",
    "        \"ROC AUC\": best_model1['roc_auc']\n",
    "    },\n",
    "    {\n",
    "        \"Model Type\": \"Post-Call Model\",\n",
    "        \"Best Model\": best_model2['model_name'],\n",
    "        \"Accuracy\": best_model2['accuracy'],\n",
    "        \"Precision\": best_model2['precision'],\n",
    "        \"Recall\": best_model2['recall'],\n",
    "        \"F1 Score\": best_model2['f1_score'],\n",
    "        \"ROC AUC\": best_model2['roc_auc']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Calculate improvements\n",
    "f1_improvement = ((best_model2['f1_score'] - best_model1['f1_score']) / best_model1['f1_score']) * 100\n",
    "accuracy_improvement = ((best_model2['accuracy'] - best_model1['accuracy']) / best_model1['accuracy']) * 100\n",
    "auc_improvement = ((best_model2['roc_auc'] - best_model1['roc_auc']) / best_model1['roc_auc']) * 100\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE IMPROVEMENTS (Post-Call vs Pre-Call):\")\n",
    "print(f\"F1 Score Improvement: {f1_improvement:.2f}%\")\n",
    "print(f\"Accuracy Improvement: {accuracy_improvement:.2f}%\")\n",
    "print(f\"ROC AUC Improvement: {auc_improvement:.2f}%\")\n",
    "\n",
    "# Business impact analysis\n",
    "print(f\"\\nüíº BUSINESS IMPACT ANALYSIS:\")\n",
    "print(f\"Pre-Call Model - Call Efficiency: {best_model1['tp']/(best_model1['tp']+best_model1['fp'])*100:.1f}%\")\n",
    "print(f\"Post-Call Model - Call Efficiency: {best_model2['tp']/(best_model2['tp']+best_model2['fp'])*100:.1f}%\")\n",
    "print(f\"\\nRecommendation: {'Use Post-Call model for better accuracy' if best_model2['f1_score'] > best_model1['f1_score'] else 'Both models perform similarly'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Recommendations\n",
    "\n",
    "In this analysis, we built two predictive models for term deposit marketing:\n",
    "\n",
    "### Pre-Call Model (Model 1)\n",
    "- **Purpose**: Predict which customers to call before making any calls\n",
    "- **Features Used**: Demographic and financial information only (excluding campaign-related features)\n",
    "- **Best Model**: Based on PyCaret model comparison and detailed evaluation\n",
    "- **Applications**: Prioritize customers for initial contact, optimize resource allocation\n",
    "- **Target Performance**: 75-80% accuracy considered high performance\n",
    "\n",
    "### Post-Call Model (Model 2)\n",
    "- **Purpose**: Predict which customers to focus on after initial contact\n",
    "- **Features Used**: All features including campaign-related ones (duration, day, month, campaign)\n",
    "- **Best Model**: Based on PyCaret model comparison and detailed evaluation\n",
    "- **Applications**: Optimize follow-up strategies, focus on high-potential customers\n",
    "\n",
    "### Key Findings\n",
    "1. **Class Imbalance**: Only ~7.5% of customers subscribe, making prediction challenging\n",
    "2. **Campaign Features Impact**: Including call duration, timing significantly improves accuracy\n",
    "3. **Business Focus**: Models prioritize avoiding unnecessary calls while capturing subscribers\n",
    "4. **PyCaret Advantage**: More comprehensive model comparison than LazyPredict\n",
    "5. **Confusion Matrices**: Always provided for business interpretation\n",
    "\n",
    "### Business Recommendations\n",
    "1. **üéØ Pre-Call Targeting**: Use Model 1 to identify high-potential customers before calling\n",
    "2. **üí∞ Cost Reduction**: Focus human resources on customers with higher subscription probability\n",
    "3. **üìû Post-Call Strategy**: Use Model 2 to determine follow-up priorities after initial contact\n",
    "4. **‚öñÔ∏è Class Imbalance**: Apply SMOTE or other techniques if performance < 75%\n",
    "5. **üìä Continuous Monitoring**: Retrain models regularly with new campaign data\n",
    "6. **üîç Root Cause Analysis**: Investigate why only 7.5% subscribe and address underlying issues\n",
    "\n",
    "### Why Only 7.5% Subscribe?\n",
    "- **Cold Calling**: Customers not expecting calls\n",
    "- **Economic Factors**: Limited disposable income for investments\n",
    "- **Poor Targeting**: Not reaching interested customer segments\n",
    "- **Product-Market Fit**: Term deposits may not meet customer needs\n",
    "- **Timing Issues**: Calls at inconvenient times\n",
    "- **Trust Factors**: Hesitation about financial products\n",
    "\n",
    "This comprehensive two-model approach with PyCaret provides better model selection, detailed confusion matrices, and business-focused metrics to optimize the marketing campaign while minimizing costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
